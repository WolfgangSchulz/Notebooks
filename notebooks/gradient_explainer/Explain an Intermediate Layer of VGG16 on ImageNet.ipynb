{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Explain an Intermediate Layer of VGG16 on ImageNet\n",
    "\n",
    "Explaining a prediction in terms of the original input image is harder than explaining the predicition in terms of a higher convolutional layer (because the higher convolutional layer is closer to the output). This notebook gives a simple example of how to use GradientExplainer to do explain a model output with respect to the 7th layer of the pretrained VGG16 network.\n",
    "\n",
    "Note that by default 200 samples are taken to compute the expectation. To run faster you can lower the number of samples per explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.keras.engine.input_layer.InputLayer object at 0x7f9b65d0b2b0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f9ba126f898>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f9b64939dd8>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f9b64904be0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f9b64904f98>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f9b64918cc0>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f9b648a66d8>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f9b648a6dd8>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f9b648b7ac8>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f9b648ce908>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f9b648d44e0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f9b648d49b0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f9b648614e0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f9b6486e2b0>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f9b64876fd0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f9b648774e0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f9b64887ef0>, <tensorflow.python.keras.layers.convolutional.Conv2D object at 0x7f9b64891da0>, <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x7f9b6489bb00>, <tensorflow.python.keras.layers.core.Flatten object at 0x7f9b6489be10>, <tensorflow.python.keras.layers.core.Dense object at 0x7f9b6482b978>, <tensorflow.python.keras.layers.core.Dense object at 0x7f9b6482ba90>, <tensorflow.python.keras.layers.core.Dense object at 0x7f9b64833160>]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1.keras.backend as K\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import shap\n",
    "# import keras.backend as K\n",
    "import json\n",
    "\n",
    "\n",
    "# explain how the input to the 7th layer of the model explains the top two classes\n",
    "def map2layer(x, layer):\n",
    "    # model.layers[0].input contains the input shape into the model (224,224,3)\n",
    "    # preprocess_input(X) contains a image\n",
    "    feed_dict = dict(zip([model.layers[0].input], [preprocess_input(x.copy())]))  # zip combines the to arrays [a,b,c] , [1,2,3] -> [(a,1),(b,2),(c,3)] - dict creates a dict from the input\n",
    "    return K.get_session().run(model.layers[layer].input, feed_dict)\n",
    "\n",
    "\n",
    "# load pre-trained model and choose two images to explain\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "print(model.layers)\n",
    "X, y = shap.datasets.imagenet50()\n",
    "# we need to load the pictures manual\n",
    "# y <- apple : 948 , strawberry : 949\n",
    "\n",
    "file1 = \"/home/jan/shap/notebooks/kernel_explainer/data/apple-with-grass.jpg\"\n",
    "img1 = image.load_img(file1, target_size=(224, 224))\n",
    "img_orig1 = image.img_to_array(img1)\n",
    "\n",
    "file2 = \"/home/jan/shap/notebooks/kernel_explainer/data/strawberry-with-grass.jpg\"\n",
    "img2 = image.load_img(file2, target_size=(224, 224))\n",
    "img_orig2 = image.img_to_array(img2)\n",
    "\n",
    "# to_explain = X[[39, 41]] (2, 224, 224, 3)\n",
    "\n",
    "to_explain = array([img_orig1, img_orig2])\n",
    "\n",
    "# load the ImageNet class names\n",
    "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
    "fname = shap.datasets.cache(url)\n",
    "with open(fname) as f:\n",
    "    class_names = json.load(f)\n",
    "\n",
    "e = shap.GradientExplainer((model.layers[7].input, model.layers[-1].output), map2layer(preprocess_input(X.copy()), 7))\n",
    "stuff = map2layer(to_explain, 7)\n",
    "shap_values, indexes = e.shap_values(stuff, ranked_outputs=2)\n",
    "\n",
    "# get the names for the classes\n",
    "index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n",
    "\n",
    "# plot the explanations\n",
    "shap.image_plot(shap_values, to_explain, index_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain with local smoothing\n",
    "\n",
    "Gradient explainer uses expected gradients, which merges ideas from integrated gradients, SHAP, and SmoothGrad into a single expection equation. To use smoothing like SmoothGrad just set the local_smoothing parameter to something non-zero. This will add normally distributed noise with that standard deviation to the input during the expectation calculation. It can create smoother feature attributions that better capture correlated regions of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# explain how the input to the 7th layer of the model explains the top two classes\n",
    "explainer = shap.GradientExplainer(\n",
    "    (model.layers[7].input, model.layers[-1].output),\n",
    "    map2layer(preprocess_input(X.copy()), 7),\n",
    "    local_smoothing=100\n",
    ")\n",
    "shap_values,indexes = explainer.shap_values(map2layer(to_explain, 7), ranked_outputs=2)\n",
    "\n",
    "# get the names for the classes\n",
    "index_names = np.vectorize(lambda x: class_names[str(x)][1])(indexes)\n",
    "\n",
    "# plot the explanations\n",
    "shap.image_plot(shap_values, to_explain, index_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}